{
  "cells": [
    {
      "cell_type": "raw",
      "source": [],
      "metadata": {
        "collapsed": false,
        "id": "m2lpLlrM3pLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object detection in video\n",
        "In this notebook we try to do object detection in a video.\n",
        "\n",
        "# Load Data\n",
        "As a first step, let's fetch the results from our training run."
      ],
      "metadata": {
        "collapsed": false,
        "id": "6OrR6FQT3pLl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-12T08:43:50.779689300Z",
          "start_time": "2023-12-12T08:43:41.503384700Z"
        },
        "id": "Gs9tiug33pLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L https://aml-2023.s3.eu-north-1.amazonaws.com/final-project/yolo_runs_epoch_90.zip > yolo_runs_epoch_90.zip"
      ],
      "metadata": {
        "id": "oxnm3wew6bH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And extract into a chosen directory."
      ],
      "metadata": {
        "collapsed": false,
        "id": "jhqkms7l3pLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "run_data_dir = \"run_data\"\n",
        "Path(run_data_dir).mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with zipfile.ZipFile(\"yolo_runs_epoch_90.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(run_data_dir)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-12T08:45:34.457827200Z",
          "start_time": "2023-12-12T08:45:32.803287400Z"
        },
        "id": "4N5rOZhC3pLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the run dataframe."
      ],
      "metadata": {
        "collapsed": false,
        "id": "gIR3b5EA3pLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "#run_results = pd.read_csv(\"data/yolo_runs_epoch_90/runs/detect/train/results.csv\")\n",
        "\n",
        "run_results = pd.read_csv(\"/content/run_data/runs/detect/train/results.csv\")\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-12T08:45:37.238698800Z",
          "start_time": "2023-12-12T08:45:37.035571700Z"
        },
        "id": "oGWAkbve3pLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Then, let's fetch the training and validation data. This we need for the validation of the YOLO model at the end."
      ],
      "metadata": {
        "collapsed": false,
        "id": "QI3DhWF13pLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O fetch_data.sh https://raw.githubusercontent.com/aml-2023/final-project/main/fetch_data.sh\n",
        "!bash fetch_data.sh --type yolo --output garbage_subset --percentage subset"
      ],
      "metadata": {
        "id": "clziB0B76dLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Train and Validation Results\n",
        "Then, we extract the training and validation columns from the dataframe."
      ],
      "metadata": {
        "collapsed": false,
        "id": "4DpGaJZ03pLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "train_columns = list(filter(lambda col_name: \"train\" in col_name, run_results.columns))\n",
        "train_results = run_results[train_columns]\n",
        "\n",
        "val_columns = list(filter(lambda col_name: \"val\" in col_name, run_results.columns))\n",
        "val_results = run_results[val_columns]"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-12T08:52:56.463747400Z",
          "start_time": "2023-12-12T08:52:56.385589800Z"
        },
        "id": "PbXWtPIA3pLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Model\n",
        "As a first step, we need to setup up the model by doing the following:\n",
        "\n",
        "1. Create a `DetectionModel` with the garbage architecture, basically just use a single class instead of the many that are normally used.\n",
        "2. Load the best weights from the training into this model.\n",
        "3. Create the YOLO model with the same best weights and with a detection task, since we want to do object detection here.\n",
        "4. Assign the detection model to the `model` field of the YOLO object. This is a bit hacky but it's the only way we can let YOLO know that it should only predict a single class."
      ],
      "metadata": {
        "collapsed": false,
        "id": "NujmiduR3pLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "!pip install ultralytics -qq\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.nn.tasks import DetectionModel\n",
        "\n",
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_j1im5oR5bV",
        "outputId": "645ff510-f077-4f00-e0b5-d95f27ee718e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/660.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/660.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m553.0/660.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m660.5/660.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "det = DetectionModel(\"/content/model.yaml\")\n",
        "det.load(torch.load(\"/content/run_data/runs/detect/train/weights/best.pt\"))\n",
        "model = YOLO(model=\"/content/run_data/runs/detect/train/weights/best.pt\", task=\"detect\")  # load a pretrained model (recommended for training)\n",
        "model.model = det"
      ],
      "metadata": {
        "id": "igZhCIxF6eva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Validation\n",
        "Next, we validate the model on the **test** data by simply calling the `val` method with the path to the `.yaml` file where we specify the dataset. This will return a `Metrics` object from which we can access all the metrics we are interested in."
      ],
      "metadata": {
        "collapsed": false,
        "id": "hPXFZ5tX3pLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the model\n",
        "data_path = os.path.abspath(\"/content/garbage_subset/data.yaml\")\n",
        "metrics = model.val(data=data_path)  # no arguments needed, dataset and settings remembered"
      ],
      "metadata": {
        "id": "f8gE9HJP6f6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trackin object in video\n",
        "- Track mode is used for tracking objects in real-time using a YOLOv8 model. In this mode, the model is loaded from a checkpoint file, and the user can provide a live video stream to perform real-time object tracking.\n",
        "\n",
        "\n",
        "Object detection in VIDEO"
      ],
      "metadata": {
        "id": "K_r54mrKVpTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = YOLO(model=\"yolov8m.pt\")"
      ],
      "metadata": {
        "id": "6WiRJUrBoxQv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Track with the model\n",
        "results1 = model.track(source=\"/content/video.mp4\", show=True)  # Tracking with default tracker"
      ],
      "metadata": {
        "id": "KH49LZS06hQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detect garbage in each frame of the video"
      ],
      "metadata": {
        "id": "3SpQ0DvM5GnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "#model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Open the video file\n",
        "video_path = \"/content/video.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    # Read a frame from the video\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    if success:\n",
        "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
        "        results = model.track(frame, persist=True)\n",
        "\n",
        "        # Visualize the results on the frame\n",
        "        annotated_frame = results[0].plot()\n",
        "\n",
        "        # Display the annotated frame\n",
        "        # cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
        "\n",
        "        # cv2.imshow is not supported so here we use cv2_imshow\n",
        "        cv2_imshow(annotated_frame)\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "yQq4R8AZxA-N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2F1Lp-E5nBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict on an image"
      ],
      "metadata": {
        "id": "nEsv7d4dV4_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results2 = model(\"/content/img1.jpeg\")  # predict on an image"
      ],
      "metadata": {
        "id": "QaYqgdTR6i8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJxO8dO5u-mX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}